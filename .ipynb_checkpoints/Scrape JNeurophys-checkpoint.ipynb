{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from requests import get\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Journal of Neurophysiology is structured such that you can find all volume and issue information hidden on any of the volume pages, so we can just start with the first volume and create a list of links for the remaining volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_url = 'https://journals.physiology.org/loi/jn/group/d1940.y1940'\n",
    "url = start_url\n",
    "year = get(url)\n",
    "year_page = bs(year.text, 'html.parser')\n",
    "all_vol = year_page.find_all('a',{'href': re.compile(r'/toc/jn/')})\n",
    "vol_list = []\n",
    "for v in all_vol:\n",
    "    try: \n",
    "        vol_str = re.search(r'Volume\\s\\d{1,3}',str(v)).group()\n",
    "        vol_int = int(vol_str[7:])\n",
    "        vol_list.append(vol_int)\n",
    "    except AttributeError:\n",
    "        print('Something bad happened.')\n",
    "\n",
    "vol_list.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default digital open access starts in Volume 77, issue 1. Perhaps start the loop from this point instead of going through all of the earlier volumes, which are all paywalled and in PDF form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_access = [i for i in vol_list if i>76]\n",
    "print(open_access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_v = 0\n",
    "base_url = 'https://journals.physiology.org/toc/jn/'\n",
    "for v in open_access:\n",
    "    # count the issues in each volume\n",
    "    if v!=last_v:\n",
    "        issue=1\n",
    "    else:\n",
    "        issue+=1\n",
    "    \n",
    "    # construct the url for the issue and load it\n",
    "    issue_url = base_url + str(v) +'/' + str(issue)\n",
    "    print(issue_url)\n",
    "    get_issue = get(issue_url)\n",
    "    issue_page = bs(get_issue.text, 'html.parser')\n",
    "    issue_date = issue_page.find('div',{'class':'col-sm-4 gray-bg toc-right-side'}).\\\n",
    "        find('span',{'class':'coverDate'}).get_text()\n",
    "\n",
    "    # scrape all of the articles in this issue\n",
    "    toc = issue_page.find_all('div',{'class':'table-of-content'})[0].find_all('div',{'class':'issue-item'})\n",
    "\n",
    "    # loop through the articles\n",
    "    for c in toc:\n",
    "        section = dict()\n",
    "        \n",
    "        # check if article is behind a paywall. If so, skip it (for now)\n",
    "        if not c.find('div',{'class':'badges'}).get_text():\n",
    "            continue\n",
    "        \n",
    "        # get article metadata\n",
    "        art.url = 'https://journals.physiology.org' + c.find('a').get('href')\n",
    "        art.title = c.find('h4').text\n",
    "        auth = c.find('ul',{'class':'rlist--inline loa'}).find_all('li')\n",
    "        art.authors = [a.get_text().replace(', and ','') for a in auth]\n",
    "        art.doi = c.select(\".epub-section__item\")[0].find('a').get('href')[16:]\n",
    "        art.id = get_next_id(db_name, table_name) # still needs to be written\n",
    "        \n",
    "        # check if this article is already in database\n",
    "        # (currently, check the title for a match)\n",
    "        # if not, add metadata to table and load article. If so, continue loop.\n",
    "        if new_article_check(art.title): # still needs to be written\n",
    "            add_new_row(db_name, table_name, art)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # load article\n",
    "        get_art = get(art.url)\n",
    "        art_page = bs(get_art, 'html.parser')\n",
    "        \n",
    "        # get reference list\n",
    "        rlist = art_page.find('ul',{'class':'rlist separator'}).find_all('li')\n",
    "        # loop through references and add them to database\n",
    "        for r in rlist:\n",
    "            r.url = [i.get('href') for i in r.find_all('a')]\n",
    "            r.title = r.find('span',{'class':'references__article-title'}).get_text()\n",
    "            r.authors = r.find('span',{'class':'references__authors'}).get_text().split(', ')\n",
    "            r.id = get_next_id(db_name, table_name) # still needs to be written\n",
    "            \n",
    "            # check if article is in database\n",
    "            if new_article_check(art.title): # still needs to be written\n",
    "                add_new_row(db_name, table_name, r)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # add the citation\n",
    "            # This function should add the citing ID to a list of citing IDs in the entry for the cited ID\n",
    "            # It should also add the cited ID to a list of cited IDs in the entry for the citing ID\n",
    "            add_citation_to_DB(citing_ID = art.id, cited_ID = r.id)\n",
    "            \n",
    "            \n",
    "        # get article sections\n",
    "        # get abstract\n",
    "        section{'Abstract'} = art_page.select('div.hlFld-Abstract div.abstractSection')[0].get_text()\n",
    "        # get all other sections\n",
    "        section['Introduction'] = ''\n",
    "        fulltext = test_page.find('div', {'class': 'hlFld-Fulltext'}).findChildren(recursive=False)\n",
    "        for f in fulltext:\n",
    "            heading = f.find('h1',{'class':'article-section__title section__title'})\n",
    "            if not heading:\n",
    "                section['Introduction']  = section['Introduction']  + ' ' + f.find_text()\n",
    "            else:\n",
    "                section[heading.get_text()] = ''\n",
    "                for text in heading.find_parent().findChildren('div'):\n",
    "                    section[heading.get_text()] = section[heading.get_text()] + text.get_text()\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Brainstorming:\n",
    "* Have a function that cycles through the DB, checks for entries with no text, and tries to find the text via PubMed (with the PubMed parser) or with Google Scholar\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing stuff out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# page = get('https://journals.physiology.org/doi/full/10.1152/jn.00104.2016')\n",
    "page = get('https://journals.physiology.org/doi/full/10.1152/jn.00399.2013')\n",
    "test_page = bs(page.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loa = test_page.find('div',{'class':'accordion-tabbed loa-accordion'}).\\\n",
    "find_all('div',{'class':'accordion-tabbed__tab-mobile '})\n",
    "[i.find('a').get_text() for i in loa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_page.select('div.cover-image__details ')[0].find('span',{'class':'volume'}).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_page.select('div.cover-image__details ')[0].find('span',{'class':'issue'}).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fulltext = test_page.find('div', {'class': 'hlFld-Fulltext'}).findChildren(recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext[7].find('h1',{'class':'article-section__title section__title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "refs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "refs[0] = dict()\n",
    "refs[0]['url'] = 'hello'\n",
    "refs[0]['title'] = 'goodbye'\n",
    "refs[1] = dict()\n",
    "refs[1]['url'] = 'bonjour'\n",
    "refs[1]['title'] = 'au revoir'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section[fulltext[7].find('h1',{'class':'article-section__title section__title'}).get_text()] = 'hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_text = fulltext[7].find('h1',{'class':'article-section__title section__title'}).find_parent().findChildren('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heading = fulltext[7].find('h1',{'class':'article-section__title section__title'})\n",
    "section[heading.get_text()] = ''\n",
    "for text in heading.find_parent().findChildren('div'):\n",
    "    section[heading.get_text()] = section[heading.get_text()] + text.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out pymysql with Amazon RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import journal_scrape as js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install -U PyMySQL\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = js.Database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dbname = \"findingssm\"\n",
    "# host = \"findingssm.c9zjgwsivgee.us-east-2.rds.amazonaws.com\"\n",
    "# port = 3306\n",
    "# user = \"danielkentwood\"\n",
    "# password = \"findingsdbsm\"\n",
    "\n",
    "# conn = pymysql.connect(host, user=user, port=port, passwd=password, db=dbname, connect_timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
